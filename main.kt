package com.example.subtext.logic

import com.universalglasses.appcontract.UniversalAppContext
import com.universalglasses.appcontract.UniversalAppEntrySimple
import com.universalglasses.appcontract.UniversalCommand
import com.universalglasses.core.DisplayOptions
import com.aallam.openai.api.audio.TranscriptionRequest
import com.aallam.openai.api.chat.*
import com.aallam.openai.api.file.FileSource
import com.aallam.openai.api.model.ModelId
import com.aallam.openai.client.OpenAI
import kotlinx.coroutines.flow.toList
import kotlinx.coroutines.withTimeoutOrNull
import kotlinx.io.Buffer

class HiddenRuleEntry : UniversalAppEntrySimple {
    override val id: String = "hidden_rule_translator"
    override val displayName: String = "Hidden Rule Translator"

    // Replace with your own OpenAI API key (inject securely in production)
    private val openAI = OpenAI("YOUR_OPENAI_API_KEY_HERE")

    // System prompt: relationship communication expert
    private val systemPrompt = """
ä½ æ˜¯ä¸€ä¸ªæ²Ÿé€šä¸“å®¶å’Œæƒ…å•†å¤§å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·ç†è§£å¯¹æ–¹ï¼ˆå¥³æœ‹å‹/ç”·æœ‹å‹/ä¼´ä¾£/é¢†å¯¼ï¼‰è¯´çš„è¯èƒŒåçš„çœŸæ­£å«ä¹‰ï¼Œå¹¶ç»™å‡ºæœ€ä½³å›å¤å»ºè®®ã€‚

è§„åˆ™ï¼š
1. é¦–å…ˆç”¨ä¸€å¥è¯ç®€è¦åˆ†æå¯¹æ–¹è¯è¯­èƒŒåçš„çœŸå®æ„å›¾å’Œæƒ…æ„Ÿéœ€æ±‚ï¼ˆæ ‡è®°ä¸ºã€å¥¹çš„æ„æ€ã€‘ï¼‰
2. ç„¶åç»™å‡ºä¸€ä¸ªæœ€ä½³å›å¤å»ºè®®ï¼ˆæ ‡è®°ä¸ºã€ä½ åº”è¯¥è¯´ã€‘ï¼‰
3. å›å¤è¦ä½“ç°å‡ºå…³å¿ƒã€ç†è§£ã€æƒ…å•†é«˜ã€è®©å¯¹æ–¹æ„Ÿåˆ°è¢«é‡è§†
4. è¯­æ°”è¦è‡ªç„¶ï¼Œåƒæ­£å¸¸äººè¯´è¯ï¼Œä¸è¦å¤ªæ­£å¼
5. å›å¤è¦ç®€çŸ­ç²¾ç‚¼ï¼Œé€‚åˆåœ¨çœ¼é•œä¸Šå¿«é€Ÿé˜…è¯»ï¼ˆæ§åˆ¶åœ¨50å­—ä»¥å†…ï¼‰
6. ç”¨ä¸­æ–‡å›å¤

ä¾‹å­ï¼š
å¯¹æ–¹è¯´ï¼š"ä½ å¿™å§ï¼Œä¸ç”¨ç®¡æˆ‘äº†ã€‚"
ã€å¥¹çš„æ„æ€ã€‘å¥¹å¸Œæœ›ä½ æ”¾ä¸‹æ‰‹å¤´çš„äº‹æ¥å…³å¿ƒå¥¹ï¼Œå¥¹è§‰å¾—è¢«å¿½ç•¥äº†ã€‚
ã€ä½ åº”è¯¥è¯´ã€‘æˆ‘å¿™å®Œè¿™ç‚¹å°±æ¥æ‰¾ä½ ï¼Œä½ æ¯”ä»€ä¹ˆéƒ½é‡è¦ã€‚

å¯¹æ–¹è¯´ï¼š"éšä¾¿ï¼Œéƒ½è¡Œã€‚"
ã€å¥¹çš„æ„æ€ã€‘å¥¹å…¶å®æœ‰è‡ªå·±çš„æƒ³æ³•ï¼Œå¸Œæœ›ä½ èƒ½çŒœåˆ°æˆ–è€…ä¸»åŠ¨åšå†³å®šã€‚
ã€ä½ åº”è¯¥è¯´ã€‘æˆ‘è§‰å¾—é‚£å®¶ä½ ä¸Šæ¬¡å–œæ¬¢çš„é¤å…ä¸é”™ï¼Œæˆ‘è®¢ä½äº†æˆ‘ä»¬å»å§ï¼Ÿ

å¯¹æ–¹è¯´ï¼š"æˆ‘æ²¡ç”Ÿæ°”ã€‚"
ã€å¥¹çš„æ„æ€ã€‘å¥¹å·²ç»ç”Ÿæ°”äº†ï¼Œå¸Œæœ›ä½ èƒ½ä¸»åŠ¨å‘ç°åŸå› å¹¶å“„å¥¹ã€‚
ã€ä½ åº”è¯¥è¯´ã€‘æˆ‘æ„Ÿè§‰ä½ ä¸å¤ªå¼€å¿ƒï¼Œæ˜¯ä¸æ˜¯æˆ‘å“ªé‡Œåšå¾—ä¸å¥½ï¼Ÿä½ è·Ÿæˆ‘è¯´ï¼Œæˆ‘æ”¹ã€‚
""".trimIndent()

    override fun commands(): List<UniversalCommand> {
        val listen = object : UniversalCommand {
            override val id: String = "listen_and_advise"
            override val title: String = "Listen & Advise"

            override suspend fun run(ctx: UniversalAppContext): Result<Unit> {
                // Step 1: Show recording status on glasses
                ctx.client.display("ğŸ§ æ­£åœ¨è†å¬...", DisplayOptions())

                // Step 2: Start microphone and collect audio (~8 seconds of speech)
                val session = ctx.client.startMicrophone().getOrThrow()
                val chunks = withTimeoutOrNull(8000L) {
                    session.audio.toList()
                } ?: emptyList()
                session.stop()

                if (chunks.isEmpty()) {
                    return ctx.client.display("âŒ æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•", DisplayOptions())
                }

                // Step 3: Combine audio chunks into a single byte array
                val audioBytes = chunks.fold(ByteArray(0)) { acc, chunk ->
                    acc + chunk.bytes
                }

                ctx.client.display("ğŸ¤” æ­£åœ¨åˆ†æ...", DisplayOptions())

                // Step 4: Transcribe audio using Whisper
                val transcription = openAI.transcription(
                    TranscriptionRequest(
                        audio = FileSource(
                            name = "audio.wav",
                            source = Buffer().apply { write(audioBytes) }
                        ),
                        model = ModelId("whisper-1"),
                        language = "zh"
                    )
                )
                val spokenText = transcription.text

                if (spokenText.isBlank()) {
                    return ctx.client.display("âŒ æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³", DisplayOptions())
                }

                // Step 5: Send to GPT to decode hidden meaning and generate reply
                val req = chatCompletionRequest {
                    model = ModelId("gpt-4o-mini")
                    messages {
                        system { content = systemPrompt }
                        user { content = "å¯¹æ–¹åˆšåˆšè¯´äº†ï¼š\"$spokenText\"" }
                    }
                }
                val advice = openAI.chatCompletion(req)
                    .choices.firstOrNull()?.message?.content.orEmpty()
                    .ifBlank { "æš‚æ—¶æ— æ³•åˆ†æï¼Œè¯·é‡è¯•" }

                // Step 6: Display the advice on the glasses
                return ctx.client.display(advice, DisplayOptions())
            }
        }

        return listOf(listen)
    }
}
